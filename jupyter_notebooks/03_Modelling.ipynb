{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Model Selection & Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "* Establish baselines; connect metrics to business trade-offs.\n",
        "\n",
        "**Note:** This modelling uses a processed stratified sample (100k rows) of a larger dataset (1M rows) to ensure efficient training while maintaining the fraud class balance. Details are provided in 01_ETL.ipynb\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Processed dataset data/processed/card_transdata_processed.csv (derived from 100k stratified sample)\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Baseline metrics, plots, and a predictions CSV for later comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I need to change the working directory from the current folder to its parent folder (required because the notebook is being run from inside the jupyter notebooks subfolder). In the code below, I change the working directory from its current folder to its parent folder.  \n",
        "* I access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chdir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1: Quick Load and Check of Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick steps to:\n",
        "- Load processed file\n",
        "- Check Dataframe Shape is as expected (100000, 17)\n",
        "- Display first 5 rows to check loads as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Import all libraries needed for the notebook\n",
        "# =============================================================================\n",
        "\n",
        "# Core data manipulation and path handling\n",
        "import pandas as pd  # Data manipulation and analysis\n",
        "import numpy as np    # Numerical operations and array handling\n",
        "from pathlib import Path  # Cross-platform file path handling\n",
        "\n",
        "# Data visualisation\n",
        "import matplotlib.pyplot as plt  # Static plotting\n",
        "import seaborn as sns # Enhanced visual styling\n",
        "\n",
        "# Display tools\n",
        "from IPython.display import display  # Pretty display of DataFrames in Jupyter\n",
        "\n",
        "# =============================================================================\n",
        "# Machine Learning: Model preparation and evaluation\n",
        "# =============================================================================\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV  # Data splitting and tuning\n",
        "from sklearn.preprocessing import StandardScaler  # Feature scaling for numeric models\n",
        "from sklearn.linear_model import LogisticRegression  # Linear baseline classifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # Simple tree model for interpretability\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Ensemble models\n",
        "from xgboost import XGBClassifier  # Gradient boosting with high performance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Model evaluation metrics\n",
        "# =============================================================================\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,  # Core classification metrics\n",
        "    precision_recall_curve, average_precision_score,  # PR curve metrics (preferred for imbalance)\n",
        "    roc_auc_score, roc_curve, confusion_matrix,  # ROC and confusion matrix\n",
        "    classification_report  # Summary table\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Load processed dataset (created from 100k stratified sample in 01 ETL.ipynb)\n",
        "# Display the shape of the dataframe\n",
        "\n",
        "df = pd.read_csv(\"data/processed/card_transdata_processed.csv\") # Load processed data\n",
        "\n",
        "df.shape # (rows, columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head() # Display first few rows of the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1.1: Data Validation and Preparation\n",
        "\n",
        "## Class Balance Verification\n",
        "\n",
        "**Purpose:** Verify that the dataset maintains the expected fraud rate (8.74%) from the stratified sampling performed in the ETL pipeline.\n",
        "\n",
        "**Expected Outcome:**\n",
        "- Fraud rate: 8.74% \n",
        "- Non-fraud rate: 91.26%\n",
        "- Imbalance ratio: approximately 1:10.4 (non-fraud to fraud)\n",
        "\n",
        "## Feature and Target Split\n",
        "\n",
        "**Purpose:** Prepare the dataset for modeling by separating features (X) and target variable (y).\n",
        "\n",
        "**Dataset Composition:**\n",
        "The dataset contains 17 columns total, split into features and target:\n",
        "\n",
        "**Excluded Features from Modelling:**\n",
        "\n",
        "1. **fraud** - Target variable (what we're predicting)\n",
        "\n",
        "2. **online_chip_category** - String categorical variable\n",
        "   - Contains labels: \"online_no_chip\", \"online_chip\", \"offline_no_chip\", \"offline_chip\"\n",
        "   - Redundant: underlying binary features (online_order and used_chip) already included\n",
        "   - Created for hypothesis testing visualisation (H8) only\n",
        "\n",
        "3. **Binned Variables** - Created for EDA visualisation only:\n",
        "   - log_distance_from_home_bin - Categorical distance ranges\n",
        "   - log_purchase_price_bin - Categorical price ratio ranges  \n",
        "   - log_distance_from_last_transaction_bin - Categorical transaction distance ranges\n",
        "   \n",
        "   **Why excluded:** Binning loses information; continuous and log-transformed versions provide superior predictive power for models\n",
        "\n",
        "**Features Retained**\n",
        "\n",
        "**Original Features:**\n",
        "- distance_from_home - Geographic distance from customer's home address (km)\n",
        "- distance_from_last_transaction - Distance from previous transaction location (km)\n",
        "- ratio_to_median_purchase_price - Current purchase relative to customer's median spending\n",
        "- repeat_retailer - Whether customer previously transacted with retailer (binary: 0/1)\n",
        "- used_chip - Chip card authentication used (binary: 0/1)\n",
        "- used_pin_number - PIN verification used (binary: 0/1)\n",
        "- online_order - Transaction channel: online vs in-store (binary: 0/1)\n",
        "\n",
        "**Log-Transformed Features:**\n",
        "- log_distance_from_home- Reduces right skew in distance distribution\n",
        "- log_distance_from_last_transaction - Handles extreme distance outliers\n",
        "- log_ratio_to_median_purchase_price - Normalises purchase ratio distribution\n",
        "\n",
        "*Rationale: Both original and log-transformed versions retained. Tree-based models (Random Forest, XGBoost) can select the most predictive representation through natural feature selection.*\n",
        "\n",
        "**Engineered Interaction Features:**\n",
        "- online_high_distance - Binary flag: online transaction far from home (combines channel + distance risk)\n",
        "- online_and_chip - Binary flag: online transaction using chip authentication\n",
        "\n",
        "**Output:**\n",
        "- **X**: Feature matrix (n_samples × 12 features) - all numeric predictors\n",
        "- **y**: Binary target vector (n_samples) - fraud indicator (0 = legitimate, 1 = fraud)\n",
        "\n",
        "**Feature Strategy:**\n",
        "This approach balances information richness with model efficiency:\n",
        "- Retains both raw and transformed features for model flexibility\n",
        "- Excludes redundant encoded categories\n",
        "- Removes low-information binned variables\n",
        "- Total of 12 features provides sufficient signal without overfitting risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA VALIDATION AND PREPARATION\n",
        "# =============================================================================\n",
        "# Validates class balance and prepares features for modeling\n",
        "# Ensures dataset integrity before model training begins\n",
        "# =============================================================================\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Class Balance Verification\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Confirm fraud rate matches expected 8.74% from stratified sampling in ETL\n",
        "# Any mismatch indicates potential data loading or processing errors\n",
        "\n",
        "# Calculate fraud statistics\n",
        "fraud_count = int(df['fraud'].sum())        # Total number of fraudulent transactions\n",
        "fraud_rate = df['fraud'].mean()             # Proportion of fraud (0 to 1)\n",
        "total_rows = df.shape[0]                    # Total number of transactions\n",
        "\n",
        "# Display fraud distribution clearly\n",
        "print(f\"Fraud rate: {fraud_rate:.2%} ({fraud_count:,} fraud cases out of {total_rows:,} total transactions)\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Verify against expected rate from ETL pipeline\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Expected rate comes from sample_log.json created during stratified sampling\n",
        "expected_fraud_rate = 0.0874  # 8.74% fraud rate from ETL stratified sampling\n",
        "rate_difference = abs(fraud_rate - expected_fraud_rate)  # Absolute difference\n",
        "\n",
        "# Check if observed rate matches expected (allow tiny floating point errors)\n",
        "if rate_difference < 0.0001:\n",
        "    print(f\"✓ Class balance verified: matches expected rate ({expected_fraud_rate:.2%})\")\n",
        "else:\n",
        "    print(f\"Issue: Fraud rate {fraud_rate:.4%} differs from expected {expected_fraud_rate:.2%}\")\n",
        "    print(f\"  Difference: {rate_difference:.4%}\")\n",
        "\n",
        "# Calculate and display imbalance ratio\n",
        "imbalance_ratio = (1 - fraud_rate) / fraud_rate  # Ratio of non-fraud to fraud\n",
        "print(f\"Imbalance ratio: 1:{imbalance_ratio:.1f} (non-fraud : fraud)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Feature and Target Split\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Separate features (X) from target variable (y)\n",
        "# Exclude features that cannot be used for modeling\n",
        "\n",
        "target = \"fraud\"  # Target variable name\n",
        "\n",
        "# Define columns to exclude from feature matrix\n",
        "# These are either:\n",
        "# 1. The target variable itself\n",
        "# 2. String categorical variables created for EDA visualization\n",
        "# 3. Binned variables created for hypothesis testing/visualization only\n",
        "exclude_cols = [\n",
        "    target,                                      # Target variable (what we're predicting)\n",
        "    'online_chip_category',                      # String labels: \"online_no_chip\", \"online_chip\", etc.\n",
        "                                                 # (underlying features online_order + used_chip already included)\n",
        "    'log_distance_from_home_bin',                # Categorical bins for EDA (continuous log_distance_from_home included)\n",
        "    'log_purchase_price_bin',                    # Categorical bins for EDA (continuous log_ratio_to_median_purchase_price included)\n",
        "    'log_distance_from_last_transaction_bin'     # Categorical bins for EDA (continuous log_distance_from_last_transaction included)\n",
        "]\n",
        "\n",
        "# Create feature matrix X (drop excluded columns if they exist)\n",
        "X = df.drop(columns=[col for col in exclude_cols if col in df.columns])\n",
        "\n",
        "# Create target vector y (ensure binary encoding: 0 or 1)\n",
        "y = df[target].astype(int)  # 0 = legitimate transaction, 1 = fraudulent transaction\n",
        "\n",
        "# Display shape and confirmation\n",
        "print(f\"Feature matrix (X) shape: {X.shape[0]:,} samples x {X.shape[1]} features\")\n",
        "print(f\"Target vector (y) shape: {y.shape[0]:,} samples\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(f\"  Class 0 (legitimate): {(y == 0).sum():,} ({(y == 0).mean():.2%})\")\n",
        "print(f\"  Class 1 (fraud):      {(y == 1).sum():,} ({(y == 1).mean():.2%})\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Display features included in modeling\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(\"Features Included in Modelling:\")\n",
        "print(f\"{'='*90}\\n\")\n",
        "\n",
        "print(\"Original Features (7 from dataset):\")\n",
        "print(\"  distance_from_home\")\n",
        "print(\"  distance_from_last_transaction\")\n",
        "print(\"  ratio_to_median_purchase_price\")\n",
        "print(\"  repeat_retailer\")\n",
        "print(\"  used_chip\")\n",
        "print(\"  used_pin_number\")\n",
        "print(\"  online_order\")\n",
        "\n",
        "print(\"\\nLog-Transformed Features (3 engineered for skew reduction):\")\n",
        "print(\"  log_distance_from_home\")\n",
        "print(\"  log_distance_from_last_transaction\")\n",
        "print(\"  log_ratio_to_median_purchase_price\")\n",
        "\n",
        "print(\"\\nInteraction Features (2 engineered for domain insights):\")\n",
        "print(\"  online_high_distance\")\n",
        "print(\"  online_and_chip\")\n",
        "\n",
        "print(f\"\\nTotal: {X.shape[1]} features\")\n",
        "print(f\"\\n{'='*90}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2: Data Splitting and Class Balance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Goal:** \n",
        "- Divide the dataset into three stratified subsets for model development, hyperparameter tuning, and final evaluation. This approach prevents data leakage and provides unbiased performance estimates.\n",
        "\n",
        "**Approach:**\n",
        "- Test set: 20% of the data, held back for final unbiased evaluation.\n",
        "- Validation set: 20% of the data, used for hyperparameter tuning and threshold selection.\n",
        "- Training set: 60% of the data, used to fit the models.\n",
        "\n",
        "Splitting was done in two steps as the set must be isolated first to prevent any information leakage during model development. If you split all three sets simultaneously, there's a risk of inadvertently using test set characteristics during validation or training decisions.\n",
        "\n",
        "Stratification is required to reserve class distribution across all splits to ensure each subset is representative of the full dataset. Without stratification, random chance could create splits with 7% fraud in training and 10% in test. This would make validation metrics unreliable predictors of test performance. \n",
        "\n",
        "**Results of Split:**\n",
        "- Train: 60,000 rows (60%)\n",
        "- Validation: 20,000 rows (20%)\n",
        "- Test: 20,000 rows (20%)\n",
        "- Fraud prevalence is 8.7% across all splits, consistent with the overall sampled dataset.\n",
        "\n",
        "**Insights:**\n",
        "- The class imbalance is preserved, meaning that validation and test performance will reflect the same challenge as the overall data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Train - Validate - Test Split\n",
        "# =============================================================================\n",
        "# Split dataset into three stratified subsets for model development and evaluation\n",
        "# Target split: 60% train / 20% validation / 20% test\n",
        "# Stratification preserves 8.74% fraud rate across all splits\n",
        "# =============================================================================\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Split Strategy: Two-Step Process\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Step 1: Hold out test set first (20%)\n",
        "#   - Test set remains completely untouched until final evaluation\n",
        "#   - Provides unbiased estimate of model performance on unseen data\n",
        "#   - No information leakage from training or validation\n",
        "#\n",
        "# Step 2: Split remaining data into train (60%) and validation (20%)\n",
        "#   - Training set: Used to fit model parameters\n",
        "#   - Validation set: Used for hyperparameter tuning and threshold optimisation\n",
        "#   - Keeps test set isolated throughout entire development process\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Step 1: Hold Out Test Set (20% of total data)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Split off 20% for final testing, keeping 80% for train/validation split\n",
        "# Test set is locked away and will not be touched until final model evaluation\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,          # Reserve 20% of total data for final testing\n",
        "    stratify=y,              # Maintain 8.74% fraud rate in both temp and test sets\n",
        "    random_state=42          # Ensures reproducibility across runs\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Step 2: Split Remaining 80% into Train (60%) and Validation (20%)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# From the 80% remaining after test split:\n",
        "# - 75% of temp becomes Training (0.75 × 0.80 = 0.60 of original)\n",
        "# - 25% of temp becomes Validation (0.25 × 0.80 = 0.20 of original)\n",
        "# Result: 60% train, 20% validation, 20% test (from Step 1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.25,          # 25% of remaining 80% = 20% of original dataset\n",
        "    stratify=y_temp,         # Maintain fraud rate in both train and validation sets\n",
        "    random_state=42          # Same seed ensures consistent splits\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Verify Split Sizes\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Confirm that split proportions match expected 60/20/20 distribution\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"  Train:\", X_train.shape)  # Expected: approximately 60,000 rows\n",
        "print(\"  Valid:\", X_val.shape)    # Expected: approximately 20,000 rows\n",
        "print(\"  Test :\", X_test.shape)   # Expected: approximately 20,000 rows\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Verify Stratification: Fraud Rate Consistency\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Confirm that fraud prevalence is consistent across all splits\n",
        "# All splits should maintain approximately 8.74% fraud rate\n",
        "\n",
        "# Verify fraud rate is consistent across all splits (should all be 8.7%)\n",
        "for name, yt in [(\"Overall\", y), (\"Train\", y_train), (\"Valid\", y_val), (\"Test\", y_test)]:\n",
        "    print(f\"{name} fraud prevalence: {yt.mean()*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2.1: Class Balance Strategy\n",
        "\n",
        "**The Challenge**\n",
        "\n",
        "Fraud represents only 8.74% of transactions (8,740 fraud cases out of 100,000 total transactions). This class imbalance presents a significant modelling challenge:\n",
        "\n",
        "Without adjustment, machine learning models optimise for overall accuracy by favouring the majority class. A baseline model could achieve 91.3% accuracy by simply predicting \"not fraud\" for every transaction, whilst failing to detect any actual fraud cases. This renders the model useless for fraud detection despite appearing highly accurate.\n",
        "\n",
        "Standard loss functions treat all misclassification errors equally. With 91.3% non-fraud cases, the model minimises overall error by predicting the majority class, treating rare fraud cases as statistical noise rather than critical signals.\n",
        "\n",
        "**Objective**\n",
        "\n",
        "Test and compare three approaches to handling class imbalance, evaluating their effectiveness for fraud detection:\n",
        "\n",
        "1. No weighting (baseline)\n",
        "2. Class weighting (class_weight=\"balanced\"`)\n",
        "3. SMOTE oversampling\n",
        "\n",
        "This systematic comparison should:\n",
        "- Demonstrate the impact of class imbalance on model performance\n",
        "- Identify the optimal approach for this dataset's moderate imbalance (8.74% fraud rate)\n",
        "- Provide empirical evidence for modelling decisions\n",
        "\n",
        "**Approach Selection Rationale**\n",
        "\n",
        "Primary Approach: \n",
        "- Class Weighting due to:\n",
        "1. Moderate imbalance: At 8.74% fraud (1:10.4 ratio), the imbalance is significant but not extreme\n",
        "2. Computational efficiency: Class weighting requires no data augmentation or resampling\n",
        "3. Preserves data distribution: Works with original samples rather than synthetic data\n",
        "4. Standard industry practice: Widely used and well-understood approach\n",
        "5. Model-native implementation: Built into sklearn models, ensuring proper integration\n",
        "\n",
        "**How it works:**\n",
        "- Assigns higher penalty to misclassifying minority class (fraud)\n",
        "- class_weight=\"balanced\" automatically calculates: weight = n_samples / (n_classes × n_samples_per_class)\n",
        "- For fraud: weight ≈ 60,000 / (2 × 5,244) ≈ 5.72\n",
        "- For non-fraud: weight ≈ 60,000 / (2 × 54,756) ≈ 0.55\n",
        "- **Relative penalty:** Fraud errors cost approximately 10× more than non-fraud errors (5.72 / 0.55 ≈ 10.4)\n",
        "\n",
        "**Secondary Approach: SMOTE Oversampling**\n",
        "- Synthetic Minority Oversampling Technique (SMOTE), selected as secondary due to:\n",
        "1. More appropriate for extreme imbalance: Most beneficial when fraud <2% of data\n",
        "2. Creates synthetic data: May not generalise as well as real samples\n",
        "3. Computational overhead: Increases training set size and fitting time\n",
        "4. Risk of overfitting: Synthetic samples might not represent true fraud patterns\n",
        "5. Better suited for severe imbalance (when insufficient minority class examples exist)\n",
        "\n",
        "**How it works:**\n",
        "- Generates synthetic fraud cases by interpolating between existing fraud samples\n",
        "- Creates new samples along line segments connecting k-nearest fraud neighbours\n",
        "- Balances training set to 50:50 or custom ratio\n",
        "\n",
        "**Expected Outcomes**\n",
        "\n",
        "Test 1: No Weighting (Baseline)\n",
        "- High overall accuracy (≈91%) from predicting majority class\n",
        "- Poor fraud detection: Low recall, likelihood of many missed fraud cases\n",
        "- Demonstrates the problem I am trying to solve\n",
        "\n",
        "Test 2: Class Weighting\n",
        "- Improved fraud detection: Higher recall at cost of more false positives\n",
        "- Better balance between catching fraud and minimising false alarms\n",
        "- Expected to be optimal for this moderate imbalance\n",
        "\n",
        "Test 3: SMOTE\n",
        "- Expect outcome to be similar (or slightly better) performance to class weighting\n",
        "- May show diminishing returns compared to class weighting\n",
        "- Testing to validate if class weighting is sufficient for this imbalance level\n",
        "\n",
        "**Success Crieria**\n",
        "\n",
        "The aim of these tests is to identify the optimal model. This will be one that:\n",
        "1. Maximises fraud detection (recall) whilst maintaining acceptable precision\n",
        "2. Achieve high PR AUC (primary metric for imbalanced data)\n",
        "3. Provide actionable probability thresholds for business decisions\n",
        "4. Balances fraud detection against investigation resource constraints and cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Test 1: No Class Weighting (Baseline without imbalance handling)\n",
        "# =============================================================================\n",
        "# Demonstrates model behaviour without handling class imbalance\n",
        "# Expected outcome: High accuracy but poor fraud detection\n",
        "# Included as it establishes the baseline problem that needs solving\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\"TEST 1: NO CLASS WEIGHTING (Baseline without imbalance handling)\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Train Logistic Regression WITHOUT Class Weighting\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Standard logistic regression treats all misclassifications equally\n",
        "# With 91.3% non-fraud cases, the model learns to predict \"not fraud\" frequently\n",
        "# This minimises overall error but fails at fraud detection\n",
        "\n",
        "logit_unweighted = LogisticRegression(\n",
        "    max_iter=2000,    # Increased from default (100) to ensure convergence; initial testing showed 1000 ensured convergence, later adjusted 2000 to match other models (no convergence warning)\n",
        "    random_state=42   # Fixed seed for reproducibility\n",
        "    # No class_weight parameter - all classes weighted equally\n",
        ")\n",
        "\n",
        "# Fit model on training data\n",
        "# Model optimises standard logistic loss without fraud-specific penalties\n",
        "\n",
        "logit_unweighted.fit(X_train, y_train)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Generate Probability Predictions on Validation Set\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Get predicted probabilities for fraud class (positive class = 1)\n",
        "# predict_proba() returns array with shape (n_samples, 2):\n",
        "#   - Column 0: Probability of non-fraud (class 0)\n",
        "#   - Column 1: Probability of fraud (class 1)\n",
        "# [:, 1] extracts only fraud probabilities for evaluation\n",
        "\n",
        "y_val_unweighted = logit_unweighted.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Calculate Performance Metrics\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# PR AUC (Precision-Recall Area Under Curve) - Primary Metric\n",
        "#   Also called Average Precision (AP)\n",
        "#   - Focuses specifically on minority class performance (fraud detection)\n",
        "#   - Summarises precision-recall trade-off across all probability thresholds\n",
        "#   - Range: 0 to 1, where higher is better\n",
        "#   - More informative than ROC AUC for imbalanced datasets\n",
        "#   - Baseline (random classifier): ≈ 0.087 (proportion of fraud cases)\n",
        "\n",
        "ap_unweighted = average_precision_score(y_val, y_val_unweighted)\n",
        "\n",
        "# ROC AUC (Receiver Operating Characteristic) - Secondary Metric\n",
        "#   - Measures overall ability to separate fraud from non-fraud\n",
        "#   - Can be misleadingly high on imbalanced data\n",
        "#   - Range: 0 to 1, where higher is better\n",
        "#   - Baseline (random classifier): 0.5\n",
        "#   - Included for comparison with standard literature\n",
        "\n",
        "roc_unweighted = roc_auc_score(y_val, y_val_unweighted)\n",
        "\n",
        "print(f\"\\nValidation Performance Metrics:\")\n",
        "print(f\"  PR AUC (Average Precision): {ap_unweighted:.3f}\")\n",
        "print(f\"  ROC AUC:                    {roc_unweighted:.3f}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Evaluate Performance at Multiple Probability Thresholds\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Default threshold (0.5) is often suboptimal for imbalanced problems (doesn't acccount for costs and risks)\n",
        "# Test common thresholds to understand precision-recall trade-off:\n",
        "#   - Lower threshold (0.5): Catch more fraud but more false alarms\n",
        "#   - Higher threshold (0.7): Fewer false alarms but miss more fraud\n",
        "\n",
        "print(\"\\nThreshold Performance:\")\n",
        "\n",
        "for t in [0.50, 0.70]:\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Convert probabilities to binary predictions using threshold t\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Decision rule: If prob_fraud >= t, predict fraud (1), else not fraud (0)\n",
        "\n",
        "    y_pred = (y_val_unweighted >= t).astype(int)\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Compute Confusion Matrix Components\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Confusion matrix layout:\n",
        "    #                      Predicted\n",
        "    #                 Not Fraud  |  Fraud\n",
        "    #     Actual  ─────────────────────────\n",
        "    #     Not Fraud    TN       |   FP\n",
        "    #     Fraud        FN       |   TP\n",
        "    #\n",
        "    # TN (True Negative):  Correctly predicted non-fraud\n",
        "    # FP (False Positive): Incorrectly flagged as fraud (Type I error)\n",
        "    #                      - False alarm, wastes investigation resources\n",
        "    # FN (False Negative): Missed fraud case (Type II error)\n",
        "    #                      - Most costly: undetected fraud causes financial loss\n",
        "    # TP (True Positive):  Correctly detects fraud cases\n",
        "     \n",
        "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Calculate Key Business Metrics\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    \n",
        "    # Precision (Positive Predictive Value):\n",
        "    #   = TP / (TP + FP)\n",
        "    #   = Of all fraud alerts, what proportion are genuine fraud?\n",
        "    #   - Low precision = many false alarms = wasted investigation effort\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    \n",
        "    # Recall (Sensitivity, True Positive Rate):\n",
        "    #   = TP / (TP + FN)\n",
        "    #   = Of all actual fraud cases, what proportion did it catch?\n",
        "    #   - Low recall = missing fraud = direct financial losses\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0 \n",
        "    \n",
        "    # Display results for this threshold\n",
        "    print(f\"\\n  Threshold {t:.2f}:\")\n",
        "    print(f\"    Recall   : {recall:.3f} ({tp}/{tp+fn} fraud cases caught)\") # High recall = catch most fraud\n",
        "    print(f\"    Precision: {precision:.3f} ({tp}/{tp+fp} predictions correct)\") # High precision = few false alarms\n",
        "    print(f\"    FP={fp:,}, FN={fn:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2.2 Test 2 - Balanced Class Weights \n",
        "\n",
        "**AIM**\n",
        "- Assess the effectiveness of class weighting for managing moderate class imbalance, and determine whether synthetic oversampling methods offer measurable performance gains.\n",
        "\n",
        "**What class_weight=\"balanced\" does:**\n",
        "- Automatically calculates weights inversely proportional to class frequencies\n",
        "- Formula: weight = n_samples / (n_classes × class_count)\n",
        "- For fraud class (8.7%): weight 5.74 (10x more importance)\n",
        "- For non-fraud class (91.3%): weight 0.55 (reduces importance)\n",
        "\n",
        "**How it works:**\n",
        "- Loss function penalises misclassified fraud cases more heavily than non-fraud\n",
        "- Forces model to prioritise catching fraud over overall accuracy\n",
        "- No data resampling required - just adjusts the optimisation objective\n",
        "\n",
        "**Expected improvement:**\n",
        "- Significantly higher recall (90-95%) compared to no weighting\n",
        "- Precision drops slightly (more false positives) but acceptable\n",
        "- Better balance between catching fraud and managing false alerts  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Test 2: Balanced Class Weights (Primary Imbalance Handling Approach)\n",
        "# =============================================================================\n",
        "# Adjusts loss function to penalise fraud misclassification more heavily\n",
        "# Expected outcome: Higher recall (catch more fraud) with acceptable precision\n",
        "# This is the preferred approach for moderate class imbalance (8.74%)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\"TEST 2: CLASS_WEIGHT='BALANCED' (Primary imbalance handling approach)\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Train Logistic Regression WITH Balanced Class Weights\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# class_weight=\"balanced\" automatically calculates inverse-frequency weights\n",
        "#\n",
        "# Formula: weight_class_i = n_samples / (n_classes × n_samples_class_i)\n",
        "#\n",
        "# For this dataset (8.74% fraud, 91.26% non-fraud):\n",
        "#   Fraud weight     ≈ 60,000 / (2 × 5,244)  ≈ 5.72\n",
        "#   Non-fraud weight ≈ 60,000 / (2 × 54,756) ≈ 0.55\n",
        "#\n",
        "# Effect on loss function:\n",
        "#   - Misclassifying fraud costs 5.72× more than baseline\n",
        "#   - Misclassifying non-fraud costs 0.55× baseline\n",
        "#   - Relative penalty: Fraud errors penalised approximately 10× more than non-fraud (5.72 / 0.55 ≈ 10.4)\n",
        "#   - Model is forced to prioritise fraud detection over overall accuracy\n",
        "\n",
        "logit_balanced = LogisticRegression(\n",
        "    class_weight=\"balanced\",  # Automatic inverse-frequency weighting\n",
        "    max_iter=2000,            # Increased to ensure convergence\n",
        "    random_state=42           # Fixed seed for reproducibility\n",
        ")\n",
        "\n",
        "# Fit model with weighted loss function\n",
        "# During training, fraud misclassifications contribute more to total loss\n",
        "# This forces model to learn better fraud detection patterns\n",
        "logit_balanced.fit(X_train, y_train)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Generate Probability Predictions on Validation Set\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Get fraud probabilities (positive class = 1)\n",
        "# predict_proba() returns [prob_non_fraud, prob_fraud]\n",
        "# [:, 1] extracts only fraud probabilities for evaluation\n",
        "\n",
        "y_val_balanced = logit_balanced.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Calculate Performance Metrics\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Expected changes compared to Test 1 (no weighting):\n",
        "#   - PR AUC: Should remain similar or improve slightly\n",
        "#   - Recall: Should increase significantly (catch more fraud)\n",
        "#   - Precision: May decrease slightly (more false positives)\n",
        "#   - Overall: Better fraud detection at cost of more false alarms\n",
        "\n",
        "ap_balanced = average_precision_score(y_val, y_val_balanced)\n",
        "roc_balanced = roc_auc_score(y_val, y_val_balanced)\n",
        "\n",
        "print(f\"\\nValidation Performance Metrics:\")\n",
        "print(f\"  PR AUC (Average Precision): {ap_balanced:.3f}\")\n",
        "print(f\"  ROC AUC:                    {roc_balanced:.3f}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Evaluate Performance at Multiple Probability Thresholds\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Test same thresholds as Test 1 for direct comparison\n",
        "# Class weighting shifts probability distribution, affecting threshold performance\n",
        "\n",
        "print(\"\\nThreshold Performance:\")\n",
        "\n",
        "for t in [0.50, 0.70]:\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Convert probabilities to binary predictions using threshold t\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Decision rule: If prob_fraud >= t, predict fraud (1), else not fraud (0)\n",
        "    \n",
        "    y_pred = (y_val_balanced >= t).astype(int)\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Compute Confusion Matrix Components\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # TN (True Negative):  Correctly predicted non-fraud\n",
        "    # FP (False Positive): False alarm - wastes investigation resources\n",
        "    # FN (False Negative): Missed fraud - most costly error (financial loss)\n",
        "    # TP (True Positive):  Correctly detected fraud - prevented loss\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Calculate Key Business Metrics\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    \n",
        "   # Precision: Of all fraud alerts, what proportion are genuine?\n",
        "    #   = TP / (TP + FP)\n",
        "    #   Expected: May decrease vs Test 1 (more false alarms acceptable trade-off)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    \n",
        "    # Recall: Of all actual fraud, what proportion did it catch?\n",
        "    #   = TP / (TP + FN)\n",
        "    #   Expected: Should increase significantly vs Test 1 (primary goal)\n",
        "    \n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    # Display results for this threshold\n",
        "    print(f\"\\n  Threshold {t:.2f}:\")\n",
        "    print(f\"    Recall   : {recall:.3f} ({tp}/{tp+fn} fraud cases caught)\")\n",
        "    print(f\"    Precision: {precision:.3f} ({tp}/{tp+fp} predictions correct)\")\n",
        "    print(f\"    FP={fp:,}, FN={fn:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2.4 Test 3 - Synthetic Minority Oversampling Technique (SMOTE)\n",
        "\n",
        "**AIM** \n",
        "- Test SMOTE as an alternative approach to handling class imbalance. This validates whether class weighting (Test 2) is sufficient or if data augmentation provides additional benefits.\n",
        "\n",
        "**What SMOTE does:**\n",
        "- Creates synthetic fraud cases by interpolating between existing fraud samples\n",
        "- Balances class distribution in training data (≈ 50/50 split after SMOTE)\n",
        "- Training set size increases from 60,000 to ≈ 110,000\n",
        "\n",
        "**How it works:**\n",
        "1. For each fraud case, find its k-nearest fraud neighbors (default k=5)\n",
        "2. Draw lines between the fraud case and its neighbors\n",
        "3. Generate new synthetic samples along those lines\n",
        "4. Result: More diverse fraud examples for the model to learn from\n",
        "\n",
        "**Trade-offs:**\n",
        "- **Pro:** Can improve recall by giving model more fraud patterns\n",
        "- **Pro:** No manual class weighting needed\n",
        "- **Con:** Increases training time (more data to process)\n",
        "- **Con:** May introduce artificial patterns not present in real data\n",
        "- **Con:** Risk of overfitting if synthetic samples are too similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST 3: SMOTE Oversampling (Secondary Imbalance Handling Approach)\n",
        "# =============================================================================\n",
        "# Creates synthetic fraud samples to balance training data\n",
        "# Expected outcome: Similar or slightly better than class weighting\n",
        "# This is the secondary approach - validates that class weighting is sufficient\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\"TEST 3: SMOTE OVERSAMPLING (Synthetic minority oversampling)\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Apply SMOTE to Training Data Only\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# SMOTE (Synthetic Minority Oversampling Technique):\n",
        "#\n",
        "# How it works:\n",
        "#   1. For each fraud case in training set, identify k=5 nearest fraud neighbours\n",
        "#   2. In feature space, draw line segments connecting the case to its neighbours\n",
        "#   3. Generate new synthetic fraud samples at random points along these lines\n",
        "#   4. Repeat until training set reaches 50:50 balance (fraud:non-fraud)\n",
        "#\n",
        "# Mathematical approach:\n",
        "#   For fraud sample x and neighbour n:\n",
        "#   synthetic_sample = x + λ × (n - x), where λ ∈ [0, 1] is random\n",
        "#   This creates realistic fraud cases between existing patterns\n",
        "#\n",
        "# Key characteristics:\n",
        "#   - Only training data is resampled (validation/test remain unchanged)\n",
        "#   - Validation performance tests generalisation to real (not synthetic) data\n",
        "#   - Creates ~52k synthetic fraud samples to match ~52k non-fraud samples\n",
        "\n",
        "smote = SMOTE(\n",
        "    random_state=42,      # Fixed seed for reproducibility\n",
        "    k_neighbors=5         # Default: use 5 nearest fraud neighbours\n",
        ")\n",
        "\n",
        "# fit_resample() generates synthetic samples and returns balanced dataset\n",
        "\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Verify SMOTE Transformation\n",
        "# Original training set: 60,000 samples (8.74% fraud = 5,244 fraud cases)\n",
        "# After SMOTE: ≈110,000 samples (50% fraud ≈ 55,000 fraud cases)\n",
        "# SMOTE added: ≈50,000 synthetic fraud samples\n",
        "\n",
        "print(f\"\\nSMOTE Transformation Summary:\")\n",
        "print(f\"  Original training shape: {X_train.shape[0]:,} samples × {X_train.shape[1]} features\")\n",
        "print(f\"  SMOTE training shape:    {X_train_smote.shape[0]:,} samples × {X_train_smote.shape[1]} features\")\n",
        "print(f\"  Samples added:           {X_train_smote.shape[0] - X_train.shape[0]:,} synthetic fraud cases\")\n",
        "print(f\"\\n  Original fraud rate: {y_train.mean()*100:.2f}%\")\n",
        "print(f\"  SMOTE fraud rate:    {y_train_smote.mean()*100:.2f}% (balanced)\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Train Logistic Regression on SMOTE-Balanced Data\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Key differences from Test 2:\n",
        "#   - Training on ≈110k samples instead of ≈60k (increased computational cost)\n",
        "#   - No class_weight needed (data already balanced at 50:50)\n",
        "#   - Model sees many synthetic fraud patterns during training\n",
        "#   - Validation tests whether synthetic samples improve real-world performance\n",
        "\n",
        "logit_smote = LogisticRegression(\n",
        "    max_iter=2000,      # Increased: larger dataset may need more iterations\n",
        "    random_state=42     # Fixed seed for reproducibility\n",
        "    # NOTE: No class_weight parameter - data is balanced via oversampling\n",
        ")\n",
        "\n",
        "# Fit model on SMOTE-resampled training data\n",
        "# Training time will be longer due to increased sample size\n",
        "logit_smote.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Evaluate on Original (Non-Resampled) Validation Set\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# CRITICAL: Validation set retains original 8.74% fraud distribution\n",
        "# This tests whether learning from synthetic samples generalises to real data\n",
        "# If SMOTE validation performance matches Test 2, class weighting is sufficient\n",
        "# If SMOTE is worse, synthetic samples may have introduced noise/overfitting\n",
        "\n",
        "# Get fraud probabilities on validation set\n",
        "# [:, 1] extracts probability of positive class (fraud)\n",
        "\n",
        "y_val_smote = logit_smote.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Calculate Performance Metrics\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Expected outcomes compared to Test 2 (class weighting):\n",
        "#   - PR AUC: Similar or slightly better (if synthetic samples add value)\n",
        "#   - Recall: Similar (both approaches prioritise fraud detection)\n",
        "#   - Precision: Similar trade-off between detection and false alarms\n",
        "#   - If results similar: validates that class weighting is sufficient\n",
        "\n",
        "ap_smote = average_precision_score(y_val, y_val_smote)\n",
        "roc_smote = roc_auc_score(y_val, y_val_smote)\n",
        "\n",
        "print(f\"\\nValidation Performance Metrics:\")\n",
        "print(f\"  PR AUC (Average Precision): {ap_smote:.3f}\")\n",
        "print(f\"  ROC AUC:                    {roc_smote:.3f}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Evaluate Performance at Multiple Probability Thresholds\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Same thresholds as Tests 1 and 2 for direct comparison\n",
        "# SMOTE may shift probability calibration due to training on synthetic data\n",
        "\n",
        "print(\"\\nThreshold Performance:\")\n",
        "\n",
        "for t in [0.50, 0.70]:\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Convert probabilities to binary predictions using threshold t\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Decision rule: If prob_fraud >= t, predict fraud (1), else not fraud (0)\n",
        "    y_pred = (y_val_smote >= t).astype(int)\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Compute Confusion Matrix Components\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # TN (True Negative):  Correctly predicted non-fraud\n",
        "    # FP (False Positive): False alarm - wastes investigation resources\n",
        "    # FN (False Negative): Missed fraud - most costly error (financial loss)\n",
        "    # TP (True Positive):  Correctly detected fraud - prevented loss\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Calculate Key Business Metrics\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    \n",
        "    # Precision: Of all fraud alerts, what proportion are genuine?\n",
        "    #   = TP / (TP + FP)\n",
        "    #   Compare to Test 2 to assess synthetic sample quality\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    \n",
        "    # Recall: Of all actual fraud, what proportion did it catch?\n",
        "    #   = TP / (TP + FN)\n",
        "    #   Should be similar to Test 2 if SMOTE adds value\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    # Display results for this threshold\n",
        "    print(f\"\\n  Threshold {t:.2f}:\")\n",
        "    print(f\"    Recall   : {recall:.3f} ({tp}/{tp+fn} fraud cases caught)\") # High recall = catch most fraud\n",
        "    print(f\"    Precision: {precision:.3f} ({tp}/{tp+fp} predictions correct)\") # High precision = few false alarms\n",
        "    print(f\"    FP={fp:,}, FN={fn:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2.5 Comparison of All Three Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Imbalance Strategy Comparison\n",
        "\n",
        "**Approach**\n",
        "\n",
        "Three imbalance handling strategies were compared using the validation set (20,000 samples, 8.74% fraud):\n",
        "- No weighting (baseline)\n",
        "- Balanced class weights (class_weight='balanced')\n",
        "- SMOTE oversampling\n",
        "\n",
        "**Summary of Results**\n",
        "\n",
        "| **Metric**           | **No Weighting**          | **Balanced Weights**      | **SMOTE**             |\n",
        "| -------------------- | ------------------------- | ------------------------- | --------------------- |\n",
        "| **PR AUC**           | **0.799**                 | 0.748                     | 0.754                 |\n",
        "| **ROC AUC**          | 0.969                     | **0.977**                 | **0.977**             |\n",
        "| **Recall @ 0.50**    | 0.658 (1,150 / 1,748)     | **0.946 (1,653 / 1,748)** | 0.944 (1,650 / 1,748) |\n",
        "| **Precision @ 0.50** | **0.813 (1,150 / 1,414)** | 0.522 (1,653 / 3,167)     | 0.532 (1,650 / 3,103) |\n",
        "| **Recall @ 0.70**    | 0.490 (857 / 1,748)       | **0.878 (1,534 / 1,748)** | 0.875 (1,530 / 1,748) |\n",
        "| **Precision @ 0.70** | **0.897 (857 / 955)**     | 0.640 (1,534 / 2,397)     | 0.646 (1,530 / 2,369) |\n",
        "\n",
        "# Key Findings\n",
        "\n",
        "**Baseline (no weighting):**\n",
        "- Highest precision (0.90 @ threshold 0.70) but lowest recall (0.49)\n",
        "- Misses half of fraud cases - unacceptable for fraud prevention\n",
        "- Strong performance suggests well-engineered features and moderate imbalance (this could be synthetic dataset limitation):\n",
        "    - These positive results may reflect limitations in the dataset due to its synthetic nature; synthetic data are often generated with balanced or well-separated feature distributions, the model may find it easier to distinguish classes even without weighting. In a real-world dataset (with noisier, overlapping fraud signals), the unweighted baseline would likely perform worse and class weighting or SMOTE would be more impactful. \n",
        "\n",
        "**Balanced weights:**\n",
        "- Major recall improvement: +39 percentage points (0.49 → 0.88 @ threshold 0.70)\n",
        "- Acceptable precision trade-off: 0.64 (down from 0.90)\n",
        "- Catches 677 additional fraud cases at cost of 765 additional false positives\n",
        "\n",
        "**SMOTE:**\n",
        "- Near-identical performance to balanced weights (recall: 0.875 vs 0.878)\n",
        "- Doubled training time with no performance gain\n",
        "- Confirms class weighting is sufficient for this imbalance level (8.74%)\n",
        "\n",
        "**Recommendation**\n",
        "- Primary approach: class_weight='balanced'\n",
        "    - Efficient, interpretable, and achieves high fraud detection\n",
        "    - Optimal for moderate imbalance (5-20% minority class)\n",
        "    - No synthetic data generation required\n",
        "\n",
        "- When SMOTE might be preferred:\n",
        "    - Extreme class imbalance (<2% minority class) \n",
        "    - Very few minority class examples (<1,000 samples)\n",
        "    - Complex fraud patterns requiring more diverse training examples\n",
        "    - When research question specifically examines synthetic oversampling efficacy \n",
        "\n",
        "Threshold recommendations:\n",
        "- Threshold 0.70 (recommended operational threshold):\n",
        "    - Recall: 87.8% - catches 1,534 of 1,748 fraud cases\n",
        "    - Precision: 64.0% - 863 false positives (manageable investigation load)\n",
        "    - Best balance between fraud detection and resource efficiency\n",
        "  \n",
        "- Threshold 0.50 (maximum fraud detection):\n",
        "     - Recall: 94.6% - catches 1,653 of 1,748 fraud cases (95 missed)\n",
        "     - Precision: 52.2% - 1,514 false positives (high investigation burden)\n",
        "     - Use when fraud prevention is paramount and resources permit\n",
        "\n",
        "- Threshold 0.80+ (precision-focused):\n",
        "   - Higher precision but significantly lower recall\n",
        "   - Only suitable if investigation capacity is severely constrained\n",
        "\n",
        "- Threshold selection guidance:\n",
        "    - Business should determine optimal threshold based on:\n",
        "        - Investigation team capacity (false positive tolerance)\n",
        "        - Cost of missed fraud vs cost of false alarms\n",
        "        - Risk appetite and regulatory requirements\n",
        "\n",
        "# Dataset Limitations: \n",
        "\n",
        "**Threshold Setting and Cost and Risk-Based Weighting (Limitation)**\n",
        "\n",
        "This analysis could not incorporate actual financial costs, which would enable more precise threshold optimisation. A complete cost-benefit framework would require:\n",
        "1. **Cost of false positives** (investigation costs):\n",
        "   - Can be estimated: average investigation time × hourly rate × FP count\n",
        "   - Example: 30 min/case × £50/hour × 863 FP = £21,575\n",
        "\n",
        "2. **Cost of false negatives** (missed fraud losses):\n",
        "   - Cannot be estimated: dataset lacks transaction amounts\n",
        "   - Critical missing information: actual financial loss per undetected fraud case\n",
        "\n",
        "Without reliable estimates of the financial loss from missed fraud or the operational cost of investigating false alerts, it was not possible to calculate truly cost-sensitive class weights.\n",
        "\n",
        "If such data were available, model weights could be set using business and risk considerations rather than relying on the automatic class_weight='balanced' option. This would align the model with real financial impact, risk appetite, and operational constraints:\n",
        "- A cost- and risk-based approach would directly link model weighting to business outcomes, penalising missed fraud more heavily and optimising recall according to real cost trade-offs.\n",
        "- In this framework, the fraud class weight could be driven by stakeholder priorities, for example:\n",
        "    - fraud_weight = (total impact of missed fraud ÷ total impact of false alert) × class imbalance ratio\n",
        "- Such an approach would ensure the model reflects the true business cost of error, balancing fraud prevention effectiveness against investigation workload and customer impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Class Imbalance Strategy Comparison\n",
        "# =============================================================================\n",
        "# Compare all three approaches side-by-side\n",
        "# Evaluates trade-offs between fraud detection (recall) and false alarms (precision)\n",
        "# Identifies optimal approach for deployment\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\"CLASS IMBALANCE STRATEGY COMPARISON (Validation Set)\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Create High-Level Metrics Comparison Table\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Compares PR AUC and ROC AUC across all three strategies\n",
        "# PR AUC is primary metric (focuses on fraud detection performance)\n",
        "# ROC AUC is secondary metric (shows overall class separability)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Strategy': ['No Weighting', 'class_weight=balanced', 'SMOTE'],\n",
        "    'PR_AUC': [ap_unweighted, ap_balanced, ap_smote],\n",
        "    'ROC_AUC': [roc_unweighted, roc_balanced, roc_smote]\n",
        "})\n",
        "\n",
        "# Display formatted comparison table\n",
        "print(\"\\nMetrics Summary:\")\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Detailed Recall and Precision Comparison at Operational Threshold\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Threshold 0.70 selected as representative operational threshold\n",
        "# This threshold typically provides good balance between:\n",
        "#   - Catching fraud (recall)\n",
        "#   - Minimising false alarms (precision)\n",
        "# Business can adjust threshold based on cost/benefit analysis\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"DETAILED PERFORMANCE COMPARISON @ Threshold 0.70\")\n",
        "print(\"=\" * 90)\n",
        "print(\"\\nThreshold 0.70 rationale:\")\n",
        "print(\"  - Balances fraud detection against investigation resource constraints\")\n",
        "print(\"  - Higher than default 0.50 to reduce false positive rate\")\n",
        "print(\"  - Commonly used in fraud detection systems\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# Iterate through all three strategies for detailed comparison\n",
        "for name, scores in [\n",
        "    ('No Weighting', y_val_unweighted),  # No class weighting applied\n",
        "    ('Balanced Weights', y_val_balanced),  # Balanced class weights\n",
        "    ('SMOTE', y_val_smote) # Synthetic minority oversampling\n",
        "]:\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Convert Probabilities to Binary Predictions at Threshold 0.70\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Decision rule: If prob_fraud >= 0.70, predict fraud (1), else not fraud (0)\n",
        "    # Higher threshold = more conservative = fewer false alarms but might miss fraud\n",
        "    y_pred = (scores >= 0.70).astype(int)\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Extract Confusion Matrix Components\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # TN (True Negative):  Correctly predicted non-fraud\n",
        "    # FP (False Positive): False alarm - investigation costs\n",
        "    # FN (False Negative): Missed fraud - financial losses\n",
        "    # TP (True Positive):  Correctly caught fraud - losses prevented\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
        "    \n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    # Calculate Business-Critical Metrics\n",
        "    # ─────────────────────────────────────────────────────────────────────────\n",
        "    \n",
        "    # Recall (Sensitivity): Of all actual fraud, what % did it catch?\n",
        "    #   = TP / (TP + FN)\n",
        "    #   Higher recall = fewer missed fraud cases = better fraud prevention\n",
        "    #   Primary goal: maximise this metric\n",
        "    recall = tp / (tp + fn)\n",
        "    \n",
        "    # Precision (Positive Predictive Value): Of all fraud alerts, what % are genuine?\n",
        "    #   = TP / (TP + FP)\n",
        "    #   Higher precision = fewer false alarms = efficient resource use\n",
        "    #   Trade-off: Higher precision often means lower recall\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    \n",
        "    # Display detailed results for this strategy\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Recall:    {recall:.3f} ({tp:,}/{tp+fn:,} fraud caught, {fn:,} missed)\") # High recall = catch most fraud\n",
        "    print(f\"  Precision: {precision:.3f} ({fp:,} false positives out of {tp+fp:,} alerts)\") # High precision = few false alarms\n",
        "    print(f\"  True Negatives: {tn:,} | False Positives: {fp:,}\") # True Negatives: correctly identified non-fraud\n",
        "    print(f\"  False Negatives: {fn:,} | True Positives: {tp:,}\") # False Negatives: missed fraud cases\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Save Comparison Results for Reporting\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Export results to CSV for use in:\n",
        "#   - Stakeholder presentations\n",
        "#   - README documentation\n",
        "#   - Business intelligence tools (Power BI, Tableau)\n",
        "#   - Model performance tracking\n",
        "\n",
        "# Ensure reports directory exists\n",
        "Path(\"reports\").mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save high-level metrics comparison\n",
        "comparison.to_csv(\"reports/balance_model_comparison.csv\", index=False)\n",
        "print(f\"\\nHigh-level metrics saved to: reports/balance_model_comparison.csv\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Create Detailed Comparison Table at Threshold 0.70\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Build comprehensive comparison including recall, precision, and confusion matrix\n",
        "\n",
        "detailed_comparison = [] # List to hold detailed results\n",
        "\n",
        "for name, scores in [ \n",
        "    ('No Weighting', y_val_unweighted), # No class weighting applied\n",
        "    ('Balanced Weights', y_val_balanced), # Balanced class weights\n",
        "    ('SMOTE', y_val_smote)\n",
        "]:\n",
        "    # Generate predictions at threshold 0.70\n",
        "    y_pred = (scores >= 0.70).astype(int) # Predict fraud if prob >= 0.70\n",
        "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel() # Confusion matrix components\n",
        "    \n",
        "    # Calculate metrics\n",
        "    recall = tp / (tp + fn) # Recall: Proportion of actual fraud caught\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0 # Precision: Proportion of alerts that are genuine fraud\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # F1 Score: Harmonic mean of precision and recall\n",
        "    \n",
        "    # Append to detailed comparison\n",
        "    detailed_comparison.append({\n",
        "        'Strategy': name,\n",
        "        'Threshold': 0.70,\n",
        "        'Recall': recall,\n",
        "        'Precision': precision,\n",
        "        'F1_Score': f1_score,\n",
        "        'True_Positives': tp,\n",
        "        'False_Positives': fp,\n",
        "        'False_Negatives': fn,\n",
        "        'True_Negatives': tn,\n",
        "        'Total_Fraud_Cases': tp + fn,\n",
        "        'Total_Alerts': tp + fp\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for easy viewing and export\n",
        "detailed_df = pd.DataFrame(detailed_comparison) # Convert list of dicts to DataFrame\n",
        "\n",
        "# Display detailed comparison\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"DETAILED COMPARISON TABLE (Threshold 0.70)\")\n",
        "print(\"=\" * 90)\n",
        "print(detailed_df.to_string(index=False))\n",
        "\n",
        "# Save detailed comparison\n",
        "detailed_df.to_csv(\"reports/balance_model_detailed_comparison_t70.csv\", index=False) # Export to CSV\n",
        "print(f\"\\nDetailed comparison saved to: reports/balance_model_detailed_comparison_t70.csv\") # Export to CSV\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Summary and Recommendation\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"SUMMARY AND RECOMMENDATION\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# Identify best strategy by PR AUC (primary metric)\n",
        "best_strategy_idx = comparison['PR_AUC'].idxmax()\n",
        "best_strategy = comparison.loc[best_strategy_idx, 'Strategy']\n",
        "best_pr_auc = comparison.loc[best_strategy_idx, 'PR_AUC']\n",
        "\n",
        "print(f\"\\nBest Strategy by PR AUC: {best_strategy} (PR AUC = {best_pr_auc:.3f})\")\n",
        "\n",
        "# Extract recall values at threshold 0.70 for comparison\n",
        "recalls_t70 = {row['Strategy']: row['Recall'] for row in detailed_comparison}\n",
        "\n",
        "print(f\"\\nRecall Comparison @ Threshold 0.70:\")\n",
        "for strategy, recall in recalls_t70.items():\n",
        "    print(f\"  {strategy:25s}: {recall:.1%}\")\n",
        "\n",
        "print(\"\\nKey Findings:\")\n",
        "print(\"  1. No Weighting achieves highest PR AUC (0.799) with strong baseline performance\")\n",
        "print(\"  2. Balanced Weights and SMOTE significantly improve recall (94%+ vs 49%)\")\n",
        "print(\"  3. Trade-off: Improved recall comes at cost of lower precision\")\n",
        "print(\"  4. SMOTE and Balanced Weights perform similarly, validating class weighting sufficiency\")\n",
        "\n",
        "print(\"\\nRecommended Approach:\")\n",
        "print(\"  • PRIMARY: class_weight='balanced'\")\n",
        "print(\"    - Reason: Achieves high recall (88-95% depending on threshold)\")\n",
        "print(\"    - No synthetic data generation required\")\n",
        "print(\"    - Computationally efficient\")\n",
        "print(\"    - Similar performance to SMOTE without added complexity\")\n",
        "print(\"\\n  - ALTERNATIVE: Consider no weighting if precision is critical\")\n",
        "print(\"    - Reason: Highest PR AUC and precision\")\n",
        "print(\"    - Accept lower recall (49-66%) for fewer false positives\")\n",
        "print(\"    - Suitable if investigation resources are highly constrained\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv (3.12.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
